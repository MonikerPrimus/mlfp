{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd10145-d8c6-4438-a577-ab729942f1ff",
   "metadata": {},
   "source": [
    "# Text mining\n",
    "\n",
    "**Data Science with AIML**<br>\n",
    "MITES Summer 2025<br>\n",
    "2025-07-02 T\n",
    "\n",
    "Two labs ago, we found a way to convert text (unstructured data) into a form more numerical, which gave us some perspectives on topics scraped from Wikipedia. In this lab, we continue that by using **text embedding** models, **dimensionality reduction**, and **clustering**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a8d7a-e47c-4eed-9b4a-8236d56ee970",
   "metadata": {},
   "source": [
    "**Imports**\n",
    "\n",
    "Make sure these packages installed. (You only need to do this once for your venv.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151a0104-d708-4154-9814-27d0c5991208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U pip matplotlib \"numpy<2\"\n",
    "# %pip install -U ipywidgets sentence-transformers\n",
    "# %pip install -U umap-learn hdbscan keybert bertopic\n",
    "# %pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caefdca-542a-42f7-9f00-faaee60c5bbd",
   "metadata": {},
   "source": [
    "These are the Python imports we're using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebdc9f-50ab-473c-b735-65b56183fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import time\n",
    "from statistics import median_high as median\n",
    "import warnings\n",
    "import random\n",
    "warnings.simplefilter(action=\"ignore\", category=(UserWarning, FutureWarning))\n",
    "\n",
    "import wikipediaapi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search\n",
    "import umap\n",
    "import hdbscan\n",
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "USER_AGENT = \"MITES/0.0 (+https://sum.mit.edu/course/mlj25)\"\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent=USER_AGENT,\n",
    "    language=\"en\",\n",
    ")\n",
    "\n",
    "wiki_cache_file = Path(\"./wiki_cache.pkl\")\n",
    "\n",
    "def get_wiki_text(page_title):\n",
    "    \"\"\"Scrape the text from a Wikipedia page\n",
    "    \n",
    "    Enhanced with caching, so we're not unnecessarily spamming\n",
    "    their servers. (:\n",
    "\n",
    "    Args:\n",
    "        page_title (str): Title of the Wikipedia article\n",
    "\n",
    "    Returns:\n",
    "        str: The text of that article\n",
    "    \"\"\"\n",
    "    if not wiki_cache_file.exists():\n",
    "        memo = {}\n",
    "    else:\n",
    "        with open(wiki_cache_file, \"rb\") as fp:\n",
    "            memo = pickle.load(fp)\n",
    "\n",
    "    if page_title not in memo:\n",
    "        page = wiki_wiki.page(page_title)\n",
    "        time.sleep(1)  # avoid spamming the server\n",
    "        memo[page_title] = page.text\n",
    "        with open(wiki_cache_file, \"wb\") as fp:\n",
    "            pickle.dump(memo, fp)\n",
    "\n",
    "    return memo[page_title]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959051e5-2f25-4c32-a96f-001de8b0c7ee",
   "metadata": {},
   "source": [
    "## Web scraping\n",
    "\n",
    "Pick your own collection of Wikipedia pages to scrape, maybe three to five. This code scrapes those pages and combines them into one long variable, `ALL_THE_TEXT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd00515-9ad0-433f-9e30-b311de2e938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_titles = [\n",
    "    \"United States\",\n",
    "    \"China\",\n",
    "    \"Russia\"\n",
    "]\n",
    "\n",
    "texts = []\n",
    "for page_title in page_titles:\n",
    "    text = get_wiki_text(page_title)\n",
    "    texts.append(text)\n",
    "\n",
    "ALL_THE_TEXT = \"\\n\\n\".join(texts)\n",
    "\n",
    "print(f\"{len(ALL_THE_TEXT)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1944f3d6-d089-4cfc-89c6-16d92ab5595f",
   "metadata": {},
   "source": [
    "Before, we tokenized things into words. We need to tokenize our text corpus into *sentences* now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cec3499-7271-484d-aa83-d9ec3ae7c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_into_sentences(text):\n",
    "    \"\"\"Tokenize a given text into a list of sentences\n",
    "\n",
    "    Args:\n",
    "        text (str): All the text\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of all sentences\n",
    "    \"\"\"\n",
    "    text = text.replace(\". \", \".\\n\\n\")\n",
    "    while \"\\n\\n\\n\" in text:\n",
    "        text = text.replace(\"\\n\\n\\n\", \"\\n\\n\")\n",
    "    return text.split(\"\\n\\n\")\n",
    "\n",
    "sentences = break_into_sentences(ALL_THE_TEXT)\n",
    "sentences = np.array(sentences)  # enhance our list into a NumPy array\n",
    "\n",
    "m = median(len(s.split(\" \")) for s in sentences)\n",
    "print(f\"{len(sentences)} sentences\")\n",
    "print(f\"Median sentence length is {m} words\")\n",
    "\n",
    "# check first five sentences\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d598a9-6bbd-4c66-a9dd-5f4f51dd72d0",
   "metadata": {},
   "source": [
    "## Text embedding\n",
    "\n",
    "Pick a sentence transformer model, any of those described [on this page at sbert.net](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#original-models). `all-MiniLM-L12-v2` is the best balance of robustness & efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56b2d7-0eb5-4de1-b939-a0e3bd8b37f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f3f3c-9af5-4fa5-915a-e0e497bc9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(embeddings.shape)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350eb84-6f11-41ae-943a-88e6afb1610a",
   "metadata": {},
   "source": [
    "### Query search\n",
    "\n",
    "How do some **search queries** work? Like when I go online and search for something. Well, since we can convert text into numbers now, we can also see *how closeby* a search query is to all our sentences, using maths. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d34368-e077-4004-bfa6-10cf94eb8861",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"life span\"\n",
    "\n",
    "query_embeddings = model.encode([query])\n",
    "results = semantic_search(query_embeddings, embeddings, top_k=5)[0]\n",
    "\n",
    "for result in results:\n",
    "    sentence = sentences[result[\"corpus_id\"]]\n",
    "    score = result[\"score\"]\n",
    "    print(f\"(score: {score})\\n{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a455c-d580-4917-a836-70297a1c03ae",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "OK but 300+ dimensions is too much.... Let's tone this down to 2 dimensions and then plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa55df-234f-4e45-afd3-8f99b9ac923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4865340-c4f9-4172-b0ec-7a64f53172e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "print(reduced_embeddings.shape)\n",
    "reduced_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f5838-a6cc-4222-964f-d7a0e554777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy title for your plot\n",
    "topics = \", \".join(page_titles)\n",
    "if len(page_titles) > 3: topics += \"...\"\n",
    "plot_title = f\"Topics: {topics}\"\n",
    "\n",
    "x = reduced_embeddings[:, 0]\n",
    "y = reduced_embeddings[:, 1]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.title(plot_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d58744-c1e8-45dd-a784-ceee6ee71e70",
   "metadata": {},
   "source": [
    "Does this shape look interesting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba4a8b-e23b-4bd9-9145-349c06782a78",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Chance are, there's *some* shape to your data, but what are those data points? We can use clustering methods to cluster the data into meaningful groups, and then we'll plot again with some color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb366617-6f19-4ede-bbed-9e20e766484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e6f6bc-9827-4c40-85d8-ff40679da63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.fit(reduced_embeddings)\n",
    "\n",
    "labels = [int(i) for i in sorted(set(clusterer.labels_))]\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a086e-4a56-4773-b91d-7067385c385f",
   "metadata": {},
   "source": [
    "Let's try plotting again, this time coloring the dots by the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1326bc-31ec-48a2-b85c-41d53d1f4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, c=clusterer.labels_)\n",
    "plt.title(plot_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c410bb2-14f1-4236-afb9-5c1f6d80148e",
   "metadata": {},
   "source": [
    "Cool! But now what even are these clusters about? Remember, each dot represents **one sentence** from your text corpus. Let's randomly sample a few from each cluster to see what they're about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda2dc0-bfd4-4a77-baa0-3c1d6b2fe54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize clusters into list\n",
    "clusters = []\n",
    "for label in labels:\n",
    "    mask = clusterer.labels_ == label\n",
    "    clusters.append(sentences[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120da91-8af2-4707-b62e-f96554201ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample k sentences from each cluster\n",
    "# (note: same sentence might appear more than once\n",
    "# for small clusters)\n",
    "k = 5\n",
    "\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    random_sentences = random.choices(cluster, k=k)\n",
    "    for sentence in random_sentences:\n",
    "        print(f\"- {sentence}\")\n",
    "    print()\n",
    "\n",
    "print(clusters[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d08cde2-8539-438a-a308-c3f5062e8423",
   "metadata": {},
   "source": [
    "## Naming the clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddbd6a9-1d0b-474d-b270-44b67b870ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"Standardize the text\n",
    "\n",
    "    Make lowercase, separate punctuation, fix spacing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to standardize\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned up text\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"!\", \"  \")\n",
    "    text = text.replace(\"?\", \"  \")\n",
    "    text = text.replace(\". \", \"  \")\n",
    "    text = text.replace(\",\", \"  \")\n",
    "    text = text.replace('''\"''', '''  ''') # Min-Jae added this\n",
    "    text = text.replace(\"(\", \"  \") # Min-Jae added this\n",
    "    text = text.replace(\")\", \"  \") # Min-Jae added this\n",
    "    text = text.replace(\" \", \"  \") # Min-Jae added this\n",
    "\n",
    "    while \"  \" in text:\n",
    "        text = text.replace(\"  \", \" \")\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Clean & tokenize the text\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list[str]: The tokenized text, as a list of str\n",
    "    \"\"\"\n",
    "    text = clean(text)\n",
    "    tokens = text.split(\" \")\n",
    "    if tokens[-1] == \"\":\n",
    "        tokens = tokens[:-1]\n",
    "    if tokens[0] == \"\":\n",
    "        tokens.pop(0)\n",
    "    return tokens\n",
    "\n",
    "stop_words = [\"the\", \"and\", \"is\", \"are\", \"of\", \"in\", \"a\", \"to\", \"as\", \"or\", \"such\", \"for\", \"at\", \"was\", \"that\", \"their\", \"can\", \"with\"]\n",
    "\n",
    "def remove_stop_words(word_list):\n",
    "    for stop_word in stop_words:\n",
    "        while stop_word in word_list:\n",
    "            word_list.remove(stop_word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff534fd-1508-47c9-9eeb-e8ad8053caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most common words in each cluster\n",
    "for i, cluster in enumerate(clusters):\n",
    "    word_counter = {}\n",
    "    \n",
    "    for sentence in cluster:\n",
    "        cleaned = clean(sentence)\n",
    "        tokens = tokenize(cleaned)\n",
    "        unstopped_tokens = remove_stop_words(tokens)\n",
    "        \n",
    "        for word in unstopped_tokens:\n",
    "            if word not in word_counter:\n",
    "                word_counter[word] = 0\n",
    "            word_counter[word] += 1\n",
    "            \n",
    "    sorted_words = sorted(word_counter, key=lambda word: word_counter[word], reverse=True)\n",
    "    word_counter = {word: word_counter[word] for word in sorted_words}\n",
    "    word_df = pd.DataFrame({\"Word\": word_counter.keys(), \"Frequency\": word_counter.values()})\n",
    "    print(f\"Cluster {i} Name:\")\n",
    "    print(*word_df.head(3)[\"Word\"].values)\n",
    "    print()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
